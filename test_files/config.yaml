debug: True # 调试模式
proxy: ""   # 代理配置
headers: # 网页header
  Cookie: somecookie
  User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36
gau:
  enable: True # 启用gau
  include_sub: True # 是否包含子域名
  with_spider: True # 是否再爬虫获取更多url
spider: # 爬虫设置
  concurrency: 500 # 爬虫并发数量
  limit: 30 # 每秒限制并发数量
  timeout: 20
  max_depth: 12                     # 最大页面深度限制
  max_page_visit_per_site: 5000     # 每个站点最多访问的页面数量
  crawler_priority: depth-first # 爬虫优先级算法 depth-first 深度优先 breadth-first 广度优先
  no_scope: False # 指定此参数，爬虫将不受范围限制，但是受最大深度限制
  only_root_scope: False  # 只限制根域名范围，如www.baidu.com，将限制爬虫范围为 *.baidu.com
  similarity_url: # 启用Url泛化过滤相同网站
    use: False  # 是否启用
    threshold: 10 #url泛化阈值
  similarity_page_dom: # 启用DOM相似度算法过滤相同网站
    use: True # 是否启用
    threshold: 5 # 网站阈值，同个domain相似度大于这个数开启过滤
    similarity: 0.95 # 相似度阈值，大于这个数判定相似
    vector_dim: 5000 # 向量维度
  simile_hash: # 简单hash算法去重
    use: False
  sources: # 使用哪些源获得更多url
    - robotstxt
    - sitemapxml
  directory_dict: ./dict # 目录字典，会扫描相关目录
  black_list: # 黑名单
    - ".+\\.google\\.com"
    - ".+\\.facebook\\.com"
scan: # 扫描器配置
  concurrency: 300 # 扫描器并发数量
  limit: 30 # 每秒限制并发数量
  filter_threshold: 10 # 防止重复的xss报告，每个域名xss最多报告的阈值
  found_hidden_parameter: True # 发现隐藏参数
  parameter_group_size: 20
  timeout: 30 # 超时时间 单位(秒)
  position: # 扫描参数位置，可选 get,post,uri,header,cookie
    - get
    - post
    - uri
  output: # 支持生成json和markdown格式，为空则不生成
    response: False # 保存返回包
    response_header: True # 保存返回header
  hidden_parameters: # 内置用于GET探测的隐藏参数
    - key
    - redirect
    - region
    - action
    - l
report: # 结果推送,填写对应参数则使用，为空则不使用
  finish_notify: True # 扫描完毕通知
  node_name: 节点001 # 节点的名称，通知的时候会带上
  bark:
    server:
    token:
  dingding:
    token:
    secret:
  feishu:
    token:
    secret: